---
title: "Homework 6"
author: "YutingMei"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  github_document
---

```{r global options, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)
```

```{r}
library('randomForest')  ## fit random forest
library('dplyr')    ## data manipulation
library('caret') ## 'createFolds'
```
* Using the “vowel.train” data, and the “randomForest” function in the R package “randomForest”. Develop a random forest classifier for the vowel data by doing the following:

* Convert the response variable in the “vowel.train” data frame to a factor variable prior to training, so that “randomForest” does classification rather than regression.

```{r}
# get vowel.train
vowel_train <- 
  read.table(url(
    'https://web.stanford.edu/~hastie/ElemStatLearn/datasets/vowel.train'), header=TRUE, sep = ',')
```

```{r}
vowel_train = vowel_train[,-1]
# vowel_train
```

```{r}
vowel_train$y = factor(vowel_train$y)
```

* Review the documentation for the “randomForest” function.
```{r}
?randomForest()
```

* Fit the random forest model to the vowel data using all of the 11 features using the default values of the tuning parameters.
```{r}
fit <- randomForest(y ~ ., data=vowel_train, 
                    proximity=TRUE)
```

```{r}
print(fit)
```

* Use 5-fold CV and tune the model by performing a grid search for the following tuning parameters: 1) the number of variables randomly sampled as candidates at each split; consider values 3, 4, and 5, and 2) the minimum size of terminal nodes; consider a sequence (1, 5, 10, 20, 40, and 80).
```{r}
set.seed(1985)
vow_flds  <- createFolds(vowel_train$y, k=5)
print(vow_flds)
```

```{r}
cvrandf <- function(nodesize, mtry) {
  cverr <- rep(NA, length(vow_flds))
  for(tst_idx in 1:length(vow_flds)) { ## for each fold
    
    ## get training and testing data
    vow_trn <- vowel_train[-vow_flds[[tst_idx]],]
    vow_tst <- vowel_train[ vow_flds[[tst_idx]],]

    
    ## fit rf model to training data
    radf_fit = randomForest(y ~ ., data=vow_trn, 
                    proximity=TRUE, nodesize = nodesize, mtry = mtry)
    
    ## compute test error on testing data
    pre_tst <- predict(radf_fit, vow_tst)
    cverr[tst_idx] <- table(mapply(`%in%`, pre_tst, vow_tst)) / length(pre_tst)
  }
  return(cverr)
}
```

```{r}
nodel = c(1, 5, 10, 20, 40, 80)
mtryl = c(3, 4, 5)
df_all = NULL
nl = c()
ml = c()
for (i in nodel)
  for (j in mtryl){
    df_all$cv_error = cvrandf(i, j)
    names(df_all) = paste0('nodel_', i, 'mtry_', j)
    nl = append(nl, i)
    ml = append(ml, j)
  }
```

```{r}
df_all = df_all %>% data.frame()
cnames = paste0('nodel_', nl, '_mtry_', ml)
colnames(df_all) = cnames
df_all
```

```{r}
sapply(df_all, mean)
```

```{r}
sapply(df_all, sd)
```

* With the tuned model, make predictions using the majority vote method, and compute the misclassification rate using the ‘vowel.test’ data.
```{r}
vowel_test <- 
  read.table(url(
    'https://web.stanford.edu/~hastie/ElemStatLearn/datasets/vowel.test'), header=TRUE, sep = ',')
```

```{r}
which.min(sapply(df_all, mean))
```

```{r}
# FALSE is the rate of model make classification correctly
# the misclassification rate is 0.1688312
ft_tuned = randomForest(y ~ ., data=vowel_train, 
                    proximity=TRUE, nodesize = 1, mtry = 3)
pre_tuned <- predict(ft_tuned, vowel_test)
mis_error <- table(mapply(`%in%`, pre_tuned, vowel_test)) / length(pre_tuned)
mis_error
```


