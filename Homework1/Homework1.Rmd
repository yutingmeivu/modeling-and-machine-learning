---
title: "Homework 1"
author: "YutingMei"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  github_document
---

```{r global options, include = FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE, cache=TRUE)
library('class')
library('dplyr')
```

* Read the help file for R's built-in linear regression function lm

```{r}
?lm
```

```{r}
## load binary classification example data from author website 
## 'ElemStatLearn' package no longer available
load(url('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/ESL.mixture.rda'))
dat <- ESL.mixture
```

```{r}

plot_mix_data <- expression({
  plot(dat$x[,1], dat$x[,2],
       col=ifelse(dat$y==0, 'blue', 'orange'),
       pch=20,
       xlab=expression(x[1]),
       ylab=expression(x[2]))
  ## draw Bayes (True) classification boundary
  prob <- matrix(dat$prob, length(dat$px1), length(dat$px2))
  cont <- contourLines(dat$px1, dat$px2, prob, levels=0.5)
  rslt <- sapply(cont, lines, col='purple')
})

eval(plot_mix_data)
```


```{r}
## fit linear classifier
fit_lc <- function(y, x) {
  x <- cbind(1, x)
  beta <- drop(solve(t(x)%*%x)%*%t(x)%*%y)
}

## make predictions from linear classifier
predict_lc <- function(x, beta) {
  cbind(1, x) %*% beta
}

## fit model to mixture data and make predictions
lc_beta <- fit_lc(dat$y, dat$x)
lc_pred <- predict_lc(dat$xnew, lc_beta)

## reshape predictions as a matrix
lc_pred <- matrix(lc_pred, length(dat$px1), length(dat$px2))
contour(lc_pred,
      xlab=expression(x[1]),
      ylab=expression(x[2]))


## find the contours in 2D space such that lc_pred == 0.5
lc_cont <- contourLines(dat$px1, dat$px2, lc_pred, levels=0.5)

## plot data and decision surface
eval(plot_mix_data)
sapply(lc_cont, lines)
```
* Re-write the functions fit_lc and predict_lc using lm, and the associated predict method for lm objects.

```{r}
dn = data.frame(x1 = dat$x[,1], x2 = dat$x[,2], y = dat$y)
```

```{r}
fit_m = function(data){
  lm(y ~ x1 + x2, data)
}

predict_m = function(fit){
  predict(fit, dat$xnew)
}
```

```{r}
# estimated parameter:
# beta1: -0.02264, beta2: 0.24960, intercept: 0.32906
re = fit_m(dn)
summary(re)
```

```{r}
# make prediction from the revised function
re_p = predict_m(re)
## reshape predictions as a matrix
re_p <- matrix(re_p, length(dat$px1), length(dat$px2))
contour(re_p,
      xlab=expression(x[1]),
      ylab=expression(x[2]))


## find the contours in 2D space such that lc_pred == 0.5
re_cont <- contourLines(dat$px1, dat$px2, re_p, levels=0.5)

## plot data and decision surface
eval(plot_mix_data)
sapply(re_cont, lines)
```

* Consider making the linear classifier more flexible, by adding squared terms for x1 and x2 to the linear model

```{r}
fit2 = lm(y ~ x1 + x2 + I(x1^2) + I(x2^2), data = dn)
```

```{r}
summary(fit2)
```

```{r}
# compare MSE(bias^2 + variance + irreducible error)
# the new linear model have smaller MSE
mse1 = mean(residuals(re)^2)
mse2 = mean(residuals(fit2)^2)
mse1
mse2
```

```{r}
pred_new = matrix(predict(fit2, dat$xnew), length(dat$px1), length(dat$px2))
contour(pred_new,
      xlab=expression(x[1]),
      ylab=expression(x[2]))

eval(plot_mix_data)
l = contourLines(dat$px1, dat$px2, pred_new, levels=0.5)
sapply(l, lines)
```


```{r}
## fit knn classifier
## use 5-NN to estimate probability of class assignment
knn_fit <- knn(train=dat$x, test=dat$xnew, cl=dat$y, k=5, prob=TRUE)
knn_pred <- attr(knn_fit, 'prob')
knn_pred <- ifelse(knn_fit == 1, knn_pred, 1-knn_pred)

## reshape predictions as a matrix
knn_pred <- matrix(knn_pred, length(dat$px1), length(dat$px2))
contour(knn_pred,
        xlab=expression(x[1]),
        ylab=expression(x[2]),
        levels=c(0.2, 0.5, 0.8))


## find the contours in 2D space such that knn_pred == 0.5
knn_cont <- contourLines(dat$px1, dat$px2, knn_pred, levels=0.5)

## plot data and decision surface
eval(plot_mix_data)
sapply(knn_cont, lines)
```

```{r}
## do bootstrap to get a sense of variance in decision surface
resample <- function(dat) {
  idx <- sample(1:length(dat$y), replace = T)
  dat$y <- dat$y[idx]
  dat$x <- dat$x[idx,]
  return(dat)
}
```

```{r}
## plot linear classifier for three bootstraps
par(mfrow=c(1,3))
for(b in 1:3) {
  datb <- resample(dat)
  ## fit model to mixture data and make predictions
  lc_beta <- fit_lc(datb$y, datb$x)
  lc_pred <- predict_lc(datb$xnew, lc_beta)
  
  ## reshape predictions as a matrix
  lc_pred <- matrix(lc_pred, length(datb$px1), length(datb$px2))

  ## find the contours in 2D space such that lc_pred == 0.5
  lc_cont <- contourLines(datb$px1, datb$px2, lc_pred, levels=0.5)
  
  ## plot data and decision surface
  eval(plot_mix_data)
  sapply(lc_cont, lines)
}
```
<br>
* Describe how this more flexible model affects the bias-variance tradeoff 
<br>
The expected squared error(or MSE) of an estimator $\hat {\theta }$ with respect to an unknown parameter $\theta$ is defined as
<br>
$MSE = E_{\theta}[(\hat \theta - \theta)^2] = E_{\theta}[(\hat \theta - E_{\theta}[\hat \theta])^2] + (E_{\theta}[\hat \theta] - \theta)^2 = Var_{\theta}(\hat \theta) + Biase_{\theta}(\hat \theta, \theta)^2$
<br>
From the figure below, the variance of new model become larger, the bias become smaller. This is because the model become more flexible, so the border(or the hyperplane) that seperate the space become more rough and the variance of the $\hat \theta$ become larger, the variance of the MSE will become larger.
```{r}
## plot the new linear classifier for three bootstraps
par(mfrow=c(1,3))
for(b in 1:3) {
  datb <- resample(dat)
  dn = data.frame(x1 = datb$x[,1], x2 = datb$x[,2], y = datb$y)
  fitn = lm(y ~ x1 + x2 + I(x1^2) + I(x2^2), data = dn)
  ## fit model to mixture data and make predictions
  pred_new = matrix(predict(fitn, dat$xnew), length(dat$px1), length(dat$px2))

  ## find the contours in 2D space such that lc_pred == 0.5
  lc <- contourLines(datb$px1, datb$px2, pred_new, levels=0.5)
  
  ## plot data and decision surface
  eval(plot_mix_data)
  sapply(lc, lines)
}
```

```{r}
## plot 5-NN classifier for three bootstraps
par(mfrow=c(1,3))
for(b in 1:3) {
  datb <- resample(dat)
  
  knn_fit <- knn(train=datb$x, test=datb$xnew, cl=datb$y, k=5, prob=TRUE)
  knn_pred <- attr(knn_fit, 'prob')
  knn_pred <- ifelse(knn_fit == 1, knn_pred, 1-knn_pred)
  
  ## reshape predictions as a matrix
  knn_pred <- matrix(knn_pred, length(datb$px1), length(datb$px2))

  ## find the contours in 2D space such that knn_pred == 0.5
  knn_cont <- contourLines(datb$px1, datb$px2, knn_pred, levels=0.5)
  
  ## plot data and decision surface
  eval(plot_mix_data)
  sapply(knn_cont, lines)
}

## plot 20-NN classifier for three bootstraps
par(mfrow=c(1,3))
for(b in 1:3) {
  datb <- resample(dat)
  
  knn_fit <- knn(train=datb$x, test=datb$xnew, cl=datb$y, k=20, prob=TRUE)
  knn_pred <- attr(knn_fit, 'prob')
  knn_pred <- ifelse(knn_fit == 1, knn_pred, 1-knn_pred)
  
  ## reshape predictions as a matrix
  knn_pred <- matrix(knn_pred, length(datb$px1), length(datb$px2))
  
  ## find the contours in 2D space such that knn_pred == 0.5
  knn_cont <- contourLines(datb$px1, datb$px2, knn_pred, levels=0.5)
  
  ## plot data and decision surface
  eval(plot_mix_data)
  sapply(knn_cont, lines)
}
```

